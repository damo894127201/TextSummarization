##注意：

这里的设置是：输入空间与输出空间是一致的，即输入空间的词包=输出空间的词包。

这里我们并没有采取论文中的做法，我们将输入空间的词包与输出空间共享。除此之外，其它则是严格按照论文中实现。

Attention机制的作用是在解码时刻重点关注输入序列中的某些单词。其计算方式是利用上一时刻的隐状态与Encoder序列各个时刻的隐状态计算而得！

Copy模块的作用是增加那些在原文中出现单词的概率，即单词的生成概率=生成模块的概率+copy模块的概率，因为上文中出现过的重要的文本片段在交流中也常被重复使用！

CopyRNN的作用是解决OOV问题，即输入空间大于输出空间，导致模型无法生成输出空间中的词。其本质上，是扩充了输出空间的大小，使其在对每条数据解码时，真实输出空间的大小=原输出空间的大小+输入序列中那些不在输出空间的词的数量。

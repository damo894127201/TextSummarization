[data] # 原始的训练集,评估集和测试集,无法直接输入模型，需要对其分词和生成词包
train_source_sequence = ../data/source/train/source.txt
train_target_sequence = ../data/source/train/target.txt
eval_source_sequence = ../data/source/eval/source.txt
eval_target_sequence = ../data/source/eval/target.txt
test_source_sequence = ../data/source/test/source.txt
test_target_sequence = ../data/source/test/target.txt

[generate_data] # 由[data]中的数据经过分词后生成的数据,用于直接灌入模型中训练,同时产生分词器和词包
segmented_root_path = ../data/segmented
tokenizer_prefix = ../data/segmented/bpe
train_source_sequence = ../data/source/train/source.txt
train_target_sequence = ../data/source/train/target.txt
eval_source_sequence = ../data/source/eval/source.txt
eval_target_sequence = ../data/source/eval/target.txt
test_source_sequence = ../data/source/test/source.txt
test_target_sequence = ../data/source/test/target.txt
tokenizer_model = ../data/segmented/bpe.model
vocabulary = ../data/vocab.txt
source_sequence_length = ../data/segmented/source_length.txt
target_sequence_length = ../data/segmented/target_length.txt
loss_path = ../result/train/loss.txt

[params] # 模型参数
vocabulary_size = 5004
batch_size = 8
eval_batch_size = 8
test_batch_size = 8
lr = 0.0001
warmup_steps = 4000
logdir = ../log
modeldir = ../model
num_epochs = 20
attention_dimension = 128 # 单词的维度,q/k/v维度为其除以num_heads
feedforward_hidden_dimension = 256
num_blocks = 6
num_heads = 8
maxlen_source = 150
maxlen_target = 30
dropout_rate = 0.5
smoothing = 0.1
test_result = ../result/test
eval_result = ../result/eval
if_train_tokenizer = 0
max_to_keep = 3